# Morpheus Configuration
# Copy this file to .env and fill in your settings

# API Configuration
API_HOST=0.0.0.0
API_PORT=8000
# Multi-worker deployment (recommended to keep read APIs responsive during long generation)
API_WORKERS=2

# Provider Selection: openai, minimax, or deepseek
LLM_PROVIDER=minimax
# Set true to force remote LLM API calls.
# If omitted, backend will auto-enable remote mode when a matching API key is detected.
REMOTE_LLM_ENABLED=true

# MiniMax Configuration (if LLM_PROVIDER=minimax)
# Just need API Key - no Group ID required
MINIMAX_API_KEY=your-minimax-api-key
MINIMAX_MODEL=MiniMax-M2.5

# OpenAI Configuration (if LLM_PROVIDER=openai)
# OPENAI_API_KEY=your-openai-api-key
# OPENAI_MODEL=gpt-4-turbo-preview

# DeepSeek Configuration (if LLM_PROVIDER=deepseek)
# DEEPSEEK_API_KEY=your-deepseek-api-key
# DEEPSEEK_BASE_URL=https://api.deepseek.com
# DEEPSEEK_MODEL=deepseek-chat
# DEEPSEEK_CONTEXT_WINDOW_TOKENS=131072
# DEEPSEEK_MAX_TOKENS=8192

# Optional global completion cap (overridden by DEEPSEEK_MAX_TOKENS when provider=deepseek)
# LLM_MAX_TOKENS=4000
# LLM_CONTEXT_WINDOW_TOKENS=32768
# LLM_TEMPERATURE=1.5

# Embedding Configuration
EMBEDDING_MODEL=embo-01
EMBEDDING_DIMENSION=1024
# Set true to force remote embedding API calls.
# If omitted, backend follows the resolved LLM runtime mode.
REMOTE_EMBEDDING_ENABLED=false

# Database Configuration
DB_PATH=../data/novelist.db
VECTOR_DB_PATH=../data/vectors

# Search Configuration
FTS_TOP_K=30
VECTOR_TOP_K=20
HYBRID_TOP_K=30

# Agent Configuration
MAX_AGENT_ITERATIONS=5
AGENT_TIMEOUT=120

# Logging
LOG_LEVEL=INFO
# Set true to print per-request access logs (method/path/status/duration)
ENABLE_HTTP_LOGGING=true
# Optional file path for backend logs
LOG_FILE=../data/logs/app.log

# Feature Flags
# Set false to temporarily disable knowledge graph extraction and APIs.
GRAPH_FEATURE_ENABLED=true
